# éŸ³é¢‘BPMåˆ†æ®µåˆ†æä¸èŠ±æ ·æ»‘å†°æ­¥æ³•åŒ¹é…ç³»ç»Ÿï¼ˆæœ€ç»ˆä¼˜åŒ–ç‰ˆ - å¸¦åŒå›¾ä¾‹å¯è‡ªå®šä¹‰ç‰ˆï¼‰
from PyQt5.QtWidgets import QApplication, QFileDialog
import sys
import matplotlib.pyplot as plt
import numpy as np
import pyaudio
import struct
from scipy.fftpack import fft
import time
import librosa
import librosa.display
import warnings
import json
from datetime import datetime
from collections import defaultdict
import textwrap

# âœ… è§£å†³ä¸­æ–‡æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = [
    'Microsoft YaHei', 'SimHei', 'Noto Sans CJK SC', 'WenQuanYi Micro Hei',
    'Arial Unicode MS', 'DejaVu Sans'
]
plt.rcParams['axes.unicode_minus'] = False


# è‡ªåŠ¨æ¢è¡Œ
def wrap_text(text, max_width=20):
    lines = []
    for line in text.split('\n'):
        wrapped_lines = textwrap.wrap(line, width=max_width)
        lines.extend(wrapped_lines)
    return '\n'.join(lines)


# å®æ—¶éŸ³é¢‘çª—å£ï¼ˆå¯é€‰ï¼Œå¯æ³¨é‡Šæ‰ï¼‰
class AudioStream(object):
    def __init__(self):
        self.CHUNK = 1024 * 2
        self.FORMAT = pyaudio.paInt16
        self.CHANNELS = 1
        self.RATE = 44100
        self.pause = False
        self.p = pyaudio.PyAudio()
        self.stream = self.p.open(format=self.FORMAT, channels=self.CHANNELS,
                                  rate=self.RATE, input=True, output=True,
                                  frames_per_buffer=self.CHUNK)
        self.init_plots()
        self.start_plot()

    def init_plots(self):
        x = np.arange(0, 2 * self.CHUNK, 2)
        xf = np.linspace(0, self.RATE, self.CHUNK)
        self.fig, (ax1, ax2) = plt.subplots(2, figsize=(15, 7))
        self.fig.canvas.mpl_connect('button_press_event', self.onClick)
        self.line, = ax1.plot(x, np.random.rand(self.CHUNK), '-', lw=2)
        self.line_fft, = ax2.semilogx(xf, np.random.rand(self.CHUNK), '-', lw=2)
        ax1.set_title('REAL-TIME AUDIO WAVEFORM')
        ax1.set_xlabel('Samples');
        ax1.set_ylabel('Volume')
        ax1.set_ylim(0, 255);
        ax1.set_xlim(0, 2 * self.CHUNK)
        ax2.set_title('REAL-TIME AUDIO SPECTRUM')
        ax2.set_xlabel('Frequency (Hz)');
        ax2.set_ylabel('Intensity')
        ax2.set_xlim(20, self.RATE / 2)
        thismanager = plt.get_current_fig_manager()
        thismanager.window.setGeometry(5, 120, 1910, 1070)
        plt.show(block=False)

    def start_plot(self):
        print('Real-time stream started (click window to pause)')
        frame_count = 0
        start_time = time.time()
        while not self.pause:
            data = self.stream.read(self.CHUNK)
            data_int = struct.unpack(str(2 * self.CHUNK) + 'B', data)
            data_np = np.array(data_int, dtype=np.int16)[::2] + 128
            self.line.set_ydata(data_np)
            yf = fft(data_int)
            self.line_fft.set_ydata(np.abs(yf[0:self.CHUNK]) / (128 * self.CHUNK))
            self.fig.canvas.draw();
            self.fig.canvas.flush_events()
            frame_count += 1
        else:
            avg_fps = frame_count / (time.time() - start_time)
            print(f'Average frame rate: {avg_fps:.0f} FPS')
            self.exit_app()

    def exit_app(self):
        print('Real-time stream closed')
        self.stream.stop_stream()
        self.stream.close()
        self.p.terminate()

    def onClick(self, event):
        self.pause = True


# æ–‡ä»¶é€‰æ‹©
def select_audio_file():
    app = QApplication(sys.argv)
    file_path, _ = QFileDialog.getOpenFileName(
        caption="é€‰æ‹©éŸ³é¢‘æ–‡ä»¶ï¼ˆæ”¯æŒWAVå’ŒMP3æ ¼å¼ï¼‰",
        directory=".", filter="Audio Files (*.wav *.mp3);;Wave Files (*.wav);;MP3 Files (*.mp3);;All Files (*)"
    )
    app.quit()
    if file_path:
        return file_path
    else:
        print("æœªé€‰æ‹©ä»»ä½•éŸ³é¢‘æ–‡ä»¶ï¼Œç¨‹åºå°†é€€å‡º")
        sys.exit()


# èŠ±æ ·æ»‘å†°æ­¥æ³•å®šä¹‰
SKATING_STEPS = {
    "60-90": {"bpm_range": (60, 90), "steps": ["Forward Outside Bracket", "Choctaw", "Spiral Sequence"]},
    "90-110": {"bpm_range": (90, 110), "steps": ["Backward Inside Bracket", "Rocker with Arm Sweep Glide"]},
    "110-130": {"bpm_range": (110, 130),
                "steps": ["Forward Outside Bracket", "Counters", "Hydroblading", "Running Edge"]},
    "130-150": {"bpm_range": (130, 150),
                "steps": ["Twizzle Sequence", "Loop Step", "Power Glide", "Choreographic Lunge"]},
    "150-170": {"bpm_range": (150, 170),
                "steps": ["Rockerâ€“Choctaw Sequence", "Counter", "Running Edge", "Hydroblading with Body Wave"]},
    "170-190": {"bpm_range": (170, 190),
                "steps": ["Quick Twizzle Burst", "Backward Outside Bracket", "Power Slide", "Body Snap Glide"]},
    "190+": {"bpm_range": (190, 300),
             "steps": ["Twizzle + Counter Chain", "Sync Beat Hydroblading", "Body Pulse Motion"]}
}


# BPM -> æ­¥æ³•ç±»å‹
def get_step_type(bpm_value):
    for k, v in SKATING_STEPS.items():
        if k == "æ¸å¼º": continue
        min_bpm, max_bpm = v["bpm_range"]
        if min_bpm <= bpm_value <= max_bpm:
            return k
    if bpm_value > 160: return "é«˜èŠ‚å¥é€‚é…"
    if bpm_value < 60: return "ä½èŠ‚å¥é€‚é…"
    return "æœªçŸ¥"


# è·å–å»ºè®®æ­¥æ³•
def get_segment_steps(segment):
    step_type = segment['step_type']
    if step_type in SKATING_STEPS:
        return SKATING_STEPS[step_type]['steps']
    elif step_type == "é«˜èŠ‚å¥é€‚é…":
        return ["å»ºè®®åŠ å¿«åŠ¨ä½œé¢‘ç‡"]
    elif step_type == "ä½èŠ‚å¥é€‚é…":
        return ["å»ºè®®å»¶é•¿åŠ¨ä½œå¹…åº¦"]
    elif step_type == "åœé¡¿ é—´å¥":
        return ["è¿‡æ¸¡æˆ–ç»´æŒå§¿æ€"]
    return ["æ— æ¨èæ­¥æ³•"]


# åˆå¹¶åŒºé—´
def merge_similar_segments(segments, min_duration=5):
    if not segments: return []
    merged = [segments[0]]
    for s in segments[1:]:
        last = merged[-1]
        if s["step_type"] == last["step_type"] or (s["end_time"] - s["start_time"]) < min_duration:
            merged[-1]["end_time"] = s["end_time"]
            d1 = last["end_time"] - last["start_time"]
            d2 = s["end_time"] - s["start_time"]
            merged[-1]["avg_bpm"] = (last["avg_bpm"] * d1 + s["avg_bpm"] * d2) / (d1 + d2)
        else:
            merged.append(s)
    return merged


# ä¿å­˜ TXT
def save_analysis_result(audio_info, segments, save_dir="."):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    name = f"èŠ±æ ·æ»‘å†°æ­¥æ³•åˆ†æ®µåˆ†æ_{audio_info['æ–‡ä»¶å']}_{timestamp}.txt"
    path = f"{save_dir}/{name}".replace("//", "/")
    content = f"===== èŠ±æ ·æ»‘å†°éŸ³é¢‘BPMåˆ†æ®µåˆ†ææŠ¥å‘Š =====\nåˆ†ææ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\nä¸€ã€éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯\n  æ–‡ä»¶åï¼š{audio_info['æ–‡ä»¶å']}\n  é‡‡æ ·ç‡ï¼š{audio_info['é‡‡æ ·ç‡']} Hz\n  æ—¶é•¿ï¼š{audio_info['æ—¶é•¿']:.2f} ç§’\n\näºŒã€BPMåˆ†æ®µä¸æ­¥æ³•\n  å…± {len(segments)} æ®µ\n\n"
    for i, seg in enumerate(segments, 1):
        content += f"  {i}. æ—¶é—´ï¼š{seg['start_time']:.2f}-{seg['end_time']:.2f}s | BPMï¼š{seg['avg_bpm']:.2f} | ç±»å‹ï¼š{seg['step_type']}\n"
        steps = get_segment_steps(seg)
        content += "     å»ºè®®æ­¥æ³•ï¼š\n"
        for j, step in enumerate(steps, 1):
            content += f"       {j}. {step}\n"
        content += "\n"
    with open(path, 'w', encoding='utf-8') as f:
        f.write(content)
    return path


# æ ‡æ³¨æ¯ä¸ªåŒºé—´çš„æ­¥æ³•ç±»å‹
def add_step_annotations(ax, segments, times, bpms, colors):
    y_offset = 10
    for i, seg in enumerate(segments):
        step_type = seg["step_type"]
        avg_bpm = seg["avg_bpm"]
        mid_time = (seg["start_time"] + seg["end_time"]) / 2
        mid_bpm = avg_bpm
        ann_text = f"{step_type}\n({mid_bpm:.0f}BPM)"
        ax.annotate(wrap_text(ann_text, 12), xy=(mid_time, mid_bpm),
                    xytext=(0, y_offset), textcoords="offset points",
                    bbox=dict(boxstyle="round,pad=0.3", facecolor=colors[i], alpha=0.7),
                    fontsize=8, ha='center', color='black', weight='bold')


# ä¸»ç¨‹åº
if __name__ == "__main__":
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    # stream = AudioStream()
    print("æ­£åœ¨æ‰“å¼€éŸ³é¢‘æ–‡ä»¶é€‰æ‹©çª—å£...")
    audio_path = select_audio_file()
    print(f"å·²é€‰ä¸­ï¼š{audio_path}\n")

    try:
        if audio_path.lower().endswith('.mp3'):
            y, sr = librosa.load(audio_path, sr=None, mono=True, dtype=np.float32)
        else:
            y, sr = librosa.load(audio_path, sr=None, mono=True, dtype=np.float32)
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥ï¼š{e}")
        sys.exit()

    audio_duration = len(y) / sr
    audio_file_name = audio_path.split("/")[-1] if "/" in audio_path else audio_path.split("\\")[-1]
    audio_info = {
        "æ–‡ä»¶å": audio_file_name,
        "æ–‡ä»¶è·¯å¾„": audio_path,
        "é‡‡æ ·ç‡": sr,
        "æ—¶é•¿": audio_duration,
        "æ•°æ®ç±»å‹": str(y.dtype)
    }
    print("âœ… åŠ è½½æˆåŠŸ")
    print(f"ğŸ“„ æ–‡ä»¶åï¼š{audio_info['æ–‡ä»¶å']} | æ—¶é•¿ï¼š{audio_info['æ—¶é•¿']:.2f}s | é‡‡æ ·ç‡ï¼š{audio_info['é‡‡æ ·ç‡']}Hz")

    print("ğŸ” æ­£åœ¨åˆ†æ®µæ£€æµ‹BPM...")
    segment_length = 5
    hop_length = 2048
    frame_length = 4096
    samples_per_segment = int(segment_length * sr)
    num_segments = int(np.ceil(len(y) / samples_per_segment))
    segments = []

    for i in range(num_segments):
        start = i * samples_per_segment
        end = min((i + 1) * samples_per_segment, len(y))
        seg_audio = y[start:end]
        if np.max(np.abs(seg_audio)) < 0.01:
            t1 = start / sr
            t2 = end / sr
            segments.append({"start_time": t1, "end_time": t2, "avg_bpm": 0, "step_type": "åœé¡¿ é—´å¥"})
            continue
        onset = librosa.onset.onset_strength(y=seg_audio, sr=sr, hop_length=hop_length, aggregate=np.mean)
        tempo, _ = librosa.beat.beat_track(onset_envelope=onset, sr=sr, hop_length=hop_length, tightness=100)
        if isinstance(tempo, np.ndarray):
            tempo = tempo.item()
        t1 = start / sr
        t2 = end / sr
        step_type = get_step_type(tempo)
        segments.append({"start_time": t1, "end_time": t2, "avg_bpm": tempo, "step_type": step_type})
        prog = (i + 1) / num_segments * 100
        if i % max(1, num_segments // 10) == 0:
            print(f"   è¿›åº¦ï¼š{prog:.1f}% | æ—¶é—´ï¼š{t1:.1f}-{t2:.1f}s | BPMï¼š{tempo:.1f}")

    print("\nğŸ”— åˆå¹¶ç›¸ä¼¼åŒºé—´...")
    merged = merge_similar_segments(segments)
    print(f"   åˆå¹¶å‰ï¼š{len(segments)} â†’ åˆå¹¶åï¼š{len(merged)}")

    print("\nğŸ’¾ ä¿å­˜åˆ†æç»“æœ...")
    save_analysis_result(audio_info, merged)
    json_name = f"éŸ³é¢‘BPMæ•°æ®_{audio_file_name.replace('.wav', '').replace('.mp3', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    json_data = {
        "åˆ†ææ—¶é—´": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "éŸ³é¢‘ä¿¡æ¯": audio_info,
        "åˆ†æ®µç»“æœ": merged,
        "æ­¥æ³•è§„åˆ™": SKATING_STEPS
    }
    with open(json_name, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“„ TXTï¼šåˆ†ææŠ¥å‘Šå·²ä¿å­˜ | ğŸ’¾ JSONï¼š{json_name}")

    print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False

    # ç¬¬ä¸€ä¸ªå›¾å½¢ï¼šéŸ³é¢‘æ³¢å½¢
    fig1, ax1 = plt.subplots(figsize=(18, 12))
    librosa.display.waveshow(y=y, sr=sr, ax=ax1, alpha=0.6, color="#1f77b4")
    ax1.set_title(f"éŸ³é¢‘æ³¢å½¢ - {audio_file_name}", fontsize=14, pad=20)
    ax1.set_ylabel("æŒ¯å¹…", fontsize=12)
    ax1.set_xlabel("æ—¶é—´ï¼ˆç§’ï¼‰", fontsize=12)
    wave_img_name = f"éŸ³é¢‘æ³¢å½¢_{audio_file_name.replace('.wav', '').replace('.mp3', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
    plt.tight_layout(pad=4.0)
    plt.savefig(wave_img_name, dpi=300, bbox_inches="tight")
    plt.show()
    print(f"âœ… éŸ³é¢‘æ³¢å½¢å›¾è¡¨å·²ä¿å­˜ï¼š{wave_img_name}")

    # ç¬¬äºŒä¸ªå›¾å½¢ï¼šBPMå˜åŒ–æ›²çº¿ä¸åŒºé—´åˆ’åˆ†
    fig2, ax2 = plt.subplots(figsize=(18, 12))
    times = [seg["start_time"] for seg in merged]
    bpms = [seg["avg_bpm"] for seg in merged]
    colors = plt.cm.Set3(np.linspace(0, 1, len(merged)))

    ax2.plot(times, bpms, 'o-', color="#2ca02c", linewidth=2, markersize=6, markerfacecolor="#ff7f0e")
    ax2.set_title("BPMå˜åŒ–æ›²çº¿ä¸åŒºé—´åˆ’åˆ†", fontsize=14, pad=20)
    ax2.set_xlabel("æ—¶é—´ï¼ˆç§’ï¼‰", fontsize=12)
    ax2.set_ylabel("BPMå€¼", fontsize=12)
    ax2.set_ylim(0, (max(bpms) * 1.1) if bpms else 160)
    ax2.set_xlim(0, audio_duration)

    for i, seg in enumerate(merged):
        ax2.axvspan(seg["start_time"], seg["end_time"], alpha=0.15, facecolor=colors[i], linewidth=0)

    add_step_annotations(ax2, merged, times, bpms, colors)

    # ======================
    # å¯ä¿®æ”¹çš„å›¾ä¾‹èµ·å§‹åæ ‡å‚æ•°ï¼ˆè‡ªå®šä¹‰ï¼‰
    # ======================
    main_legend_x = 1.05  # ä¸»è¦åŒºé—´å›¾ä¾‹çš„æ°´å¹³ä½ç½®ï¼ˆå‘å³ï¼Œå¦‚1.05ï¼‰
    main_legend_y = 0.95  # ä¸»è¦åŒºé—´å›¾ä¾‹çš„å‚ç›´ä½ç½®ï¼ˆ0~1ï¼Œè¶Šå¤§è¶Šé ä¸Šï¼Œå¦‚0.95ï¼‰

    step_legend_x = 1.05  # BPMæ­¥æ³•å›¾ä¾‹çš„æ°´å¹³ä½ç½®ï¼ˆä¸ä¸»è¦åŒºé—´ä¸€è‡´æˆ–ä¸åŒï¼‰
    step_legend_y = 0.6  # BPMæ­¥æ³•å›¾ä¾‹çš„å‚ç›´ä½ç½®ï¼ˆ0~1ï¼Œæ¯”å¦‚0.6æ›´é ä¸‹ï¼‰
    # ======================

    # ======================
    # å›¾ä¾‹1ï¼šä¸»è¦åŒºé—´ï¼ˆå‰5ä¸ªåŒºé—´ï¼‰
    # ======================
    handles1 = []
    labels1 = []
    if len(merged) > 0:
        for i in range(min(5, len(merged))):
            color = colors[i]
            label = f"{i + 1}. {merged[i]['step_type']} ({merged[i]['avg_bpm']:.0f}BPM)"
            handles1.append(plt.Line2D([0], [0], color=color, lw=4))
            labels1.append(label)

    # åˆ›å»ºå¹¶æ·»åŠ ç¬¬ä¸€ä¸ªå›¾ä¾‹ï¼ˆä¸»è¦åŒºé—´ï¼‰
    legend1 = ax2.legend(handles1, labels1,
                         bbox_to_anchor=(main_legend_x, main_legend_y),
                         loc='upper left',
                         fontsize=9,
                         title="ä¸»è¦åŒºé—´",
                         title_fontsize=10)
    # å¿…é¡»æ·»åŠ è¿™ä¸ªï¼Œä¸ç„¶ä¼šè¢«ä¸‹ä¸€ä¸ªè¦†ç›–
    ax2.add_artist(legend1)

    # ======================
    # å›¾ä¾‹2ï¼šBPMåŒºé—´å¯¹åº”çš„èŠ±æ»‘æ­¥æ³•
    # ======================
    handles2 = []
    labels2 = []
    for idx, (range_key, step_info) in enumerate(SKATING_STEPS.items()):
        steps_text = "\n".join([
            textwrap.fill(step, width=40)
            for step in step_info["steps"]
        ])
        full_text = f"{range_key} BPM:\n{steps_text}\n{'-' * 25}"
        color = plt.cm.Set3(idx / len(SKATING_STEPS))
        handles2.append(plt.Line2D([0], [0], color=color, lw=4))
        labels2.append(full_text)

    # åˆ›å»ºå¹¶æ·»åŠ ç¬¬äºŒä¸ªå›¾ä¾‹ï¼ˆBPMæ­¥æ³•ï¼‰
    legend2 = ax2.legend(handles2, labels2,
                         bbox_to_anchor=(step_legend_x, step_legend_y),
                         loc='upper left',
                         fontsize=8,
                         title="BPMåŒºé—´å¯¹åº”æ­¥æ³•",
                         title_fontsize=10)

    
    bpm_img_name = f"BPMå¯è§†åŒ–_ä¼˜åŒ–ç‰ˆ_{audio_file_name.replace('.wav', '').replace('.mp3', '')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
    plt.tight_layout(pad=4.0)
    plt.savefig(bpm_img_name, dpi=300, bbox_inches="tight")
    plt.show()
    print(f"âœ… BPMå¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ï¼š{bpm_img_name}")

    print(f"\nğŸ‰ æ‰€æœ‰æµç¨‹å®Œæˆï¼")